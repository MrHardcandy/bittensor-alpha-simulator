#!/usr/bin/env python3
"""
ä¼˜åŒ–ä»»åŠ¡æ¢å¤è„šæœ¬
ä»ä¸Šæ¬¡ä¿å­˜çš„è¿›åº¦ç»§ç»­è¿è¡Œä¼˜åŒ–ä»»åŠ¡
"""

import os
import sys
import pickle
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, 'src')
sys.path.insert(0, src_path)
sys.path.insert(0, current_dir)

from optimizer_main import OptimizationEngine, ParameterGridGenerator

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('optimizer_resume.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def find_latest_progress_file() -> Optional[str]:
    """æŸ¥æ‰¾æœ€æ–°çš„è¿›åº¦æ–‡ä»¶"""
    progress_files = [f for f in os.listdir('.') if f.startswith('optimization_progress_') and f.endswith('.pkl')]
    
    if not progress_files:
        return None
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
    progress_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    return progress_files[0]

def load_progress(progress_file: str) -> Dict[str, Any]:
    """åŠ è½½è¿›åº¦æ•°æ®"""
    with open(progress_file, 'rb') as f:
        return pickle.load(f)

def resume_optimization(progress_file: str = None):
    """æ¢å¤ä¼˜åŒ–ä»»åŠ¡"""
    
    if not progress_file:
        progress_file = find_latest_progress_file()
    
    if not progress_file:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œæ— æ³•æ¢å¤")
        return False
    
    logger.info(f"ğŸ”„ ä»è¿›åº¦æ–‡ä»¶æ¢å¤: {progress_file}")
    
    try:
        # åŠ è½½è¿›åº¦
        progress_data = load_progress(progress_file)
        
        completed_batches = progress_data['completed_batches']
        total_batches = progress_data['total_batches']
        completed_combinations = progress_data['completed_combinations']
        
        logger.info(f"ğŸ“Š è¿›åº¦ä¿¡æ¯:")
        logger.info(f"  - å·²å®Œæˆæ‰¹æ¬¡: {completed_batches}/{total_batches}")
        logger.info(f"  - å·²å®Œæˆç»„åˆ: {completed_combinations}")
        logger.info(f"  - å®Œæˆè¿›åº¦: {(completed_batches/total_batches)*100:.1f}%")
        
        if completed_batches >= total_batches:
            logger.info("âœ… ä»»åŠ¡å·²å®Œæˆï¼Œæ— éœ€æ¢å¤")
            return True
        
        # é‡æ–°ç”Ÿæˆå‚æ•°ç½‘æ ¼
        param_grid = ParameterGridGenerator.generate_parameter_combinations()
        all_combinations = []
        all_combinations.extend(param_grid['1000_TAO'])
        all_combinations.extend(param_grid['5000_TAO'])
        
        # è®¡ç®—éœ€è¦ç»§ç»­çš„ç»„åˆ
        batch_size = 16  # ä½¿ç”¨ç›¸åŒçš„æ‰¹æ¬¡å¤§å°
        start_index = completed_batches * batch_size
        remaining_combinations = all_combinations[start_index:]
        
        logger.info(f"ğŸ“‹ å‰©ä½™ç»„åˆ: {len(remaining_combinations)}")
        
        if not remaining_combinations:
            logger.info("âœ… æ‰€æœ‰ç»„åˆå·²å®Œæˆ")
            return True
        
        # åˆ›å»ºæ–°çš„ä¼˜åŒ–å¼•æ“å®ä¾‹ï¼Œç»§ç»­å¤„ç†
        logger.info("ğŸš€ ç»§ç»­ä¼˜åŒ–ä»»åŠ¡...")
        
        # ä½¿ç”¨åŸå§‹çš„æ—¶é—´æˆ³
        original_timestamp = progress_file.split('_')[2].replace('.pkl', '')
        
        optimizer = OptimizationEngine()
        optimizer.timestamp = original_timestamp
        optimizer.results_file = f"optimization_results_{original_timestamp}.json"
        optimizer.progress_file = progress_file
        optimizer.batch_results = progress_data.get('batch_results', [])
        
        # ç»§ç»­å¤„ç†å‰©ä½™çš„æ‰¹æ¬¡
        total_combinations = len(all_combinations)
        
        for i in range(0, len(remaining_combinations), batch_size):
            batch = remaining_combinations[i:i + batch_size]
            current_batch_num = completed_batches + (i // batch_size) + 1
            
            logger.info(f"ğŸ“¦ å¤„ç†æ¢å¤æ‰¹æ¬¡ {current_batch_num}/{total_batches} ({len(batch)} ä¸ªç»„åˆ)")
            
            # è¿™é‡Œéœ€è¦å¯¼å…¥å¹¶ä½¿ç”¨workerå‡½æ•°
            from optimizer_main import run_simulation_worker
            from multiprocessing import Pool
            import time
            import gc
            
            batch_start_time = time.time()
            
            with Pool(optimizer.max_workers) as pool:
                batch_results = pool.map(run_simulation_worker, batch)
            
            batch_duration = time.time() - batch_start_time
            
            # ä¿å­˜æ‰¹æ¬¡ç»“æœ
            optimizer.save_batch_results(batch_results, current_batch_num, total_batches)
            
            logger.info(f"âœ… æ¢å¤æ‰¹æ¬¡ {current_batch_num} å®Œæˆï¼Œè€—æ—¶ {batch_duration:.1f}ç§’")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
        
        logger.info("ğŸ‰ ä¼˜åŒ–ä»»åŠ¡æ¢å¤å®Œæˆ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¢å¤å¤±è´¥: {str(e)}")
        return False

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¢å¤ä¼˜åŒ–ä»»åŠ¡")
    parser.add_argument("--progress-file", help="æŒ‡å®šè¿›åº¦æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºå¯ç”¨çš„è¿›åº¦æ–‡ä»¶")
    
    args = parser.parse_args()
    
    if args.list:
        progress_files = [f for f in os.listdir('.') if f.startswith('optimization_progress_') and f.endswith('.pkl')]
        if progress_files:
            print("ğŸ“ å¯ç”¨çš„è¿›åº¦æ–‡ä»¶:")
            for i, f in enumerate(progress_files, 1):
                stat = os.stat(f)
                size = stat.st_size
                mtime = datetime.fromtimestamp(stat.st_mtime)
                print(f"  {i}. {f} ({size} bytes, ä¿®æ”¹æ—¶é—´: {mtime})")
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è¿›åº¦æ–‡ä»¶")
        return
    
    print("ğŸ”„ ä¼˜åŒ–ä»»åŠ¡æ¢å¤å·¥å…·")
    print("=" * 40)
    
    success = resume_optimization(args.progress_file)
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()